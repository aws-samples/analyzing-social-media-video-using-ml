{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b965fe5a-b65c-4115-a68f-dedaa4d6333e",
   "metadata": {},
   "source": [
    "# Techniques for analysing, tagging & detecting breaks in social media video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b1e92-0177-49f9-878b-52dc849b0ea4",
   "metadata": {},
   "source": [
    "This notebook contains sample code that will analyse a video file and determine appropriate tags and breaks in content. It utilises Amazon Transcribe, Amazon Rekognition and Amazon Bedrock.\n",
    "\n",
    "To run this notebook will incur costs from deployment and utilisation of AWS services. Please ensure the steps followed in the 'Cleanup' section are followed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bea35-05e1-4726-9b99-c68830e9702d",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535d8aa-8d6d-4ee3-af11-caabac776971",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "#### IAM Role\n",
    "The IAM role that you are using in this notebook will require access to the following services: \n",
    "- Amazon Transcribe \n",
    "- Amazon Rekognition \n",
    "- Amazon Bedrock\n",
    "- An S3 bucket\n",
    "\n",
    "Additionally, you will need to ensure that Anthrophic Claude is enabled in your Amazon Bedrock console under \"Model Access\". \n",
    "\n",
    "The S3 bucket is where you will store your video file. This S3 bucket will also be used for temporary 'scratch space' storage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d902d48-a4fc-4d3e-a67e-887543b7520b",
   "metadata": {},
   "source": [
    "#### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install -q -y boto3 ffmpeg huggingface_hub numpy numba jinja2 sqlalchemy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91728ffb-a551-4d49-baff-93051f27d0bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3 --upgrade opencv-python-headless langchain librosa  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890d70a",
   "metadata": {},
   "source": [
    "You should reboot the kernel after installing the required dependancies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117ca55",
   "metadata": {},
   "source": [
    "#### Configure your variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5a0a5-140b-4622-aac0-f96ca6498195",
   "metadata": {},
   "source": [
    "First specify the bucket and file location of the video you wish to analyse. \n",
    "\n",
    "It is recommended to use a compressed verison of your video file for analysis as a full, high definition copy is not required. At the end of this notebook there is some example code using AWS Elemenental MediaConvert that you can use to compress your video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03d246-5acc-4403-8076-3bab2b1745a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name=\"s3-bucket-name-for-video\"\n",
    "\n",
    "key=\"myvideo-for-analysis.mp4\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3e20c9-e7aa-4b7d-9f03-6e9fdb723085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time \n",
    "import jinja2\n",
    "import boto3\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math \n",
    "from langchain.llms import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "import base64 \n",
    "import os\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "media_file_uri=\"s3://\" + bucket_name + \"/\" + key \n",
    "\n",
    "jobID=str(uuid.uuid4()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdec38f",
   "metadata": {},
   "source": [
    "## Solution Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc3914-5b7a-4062-9222-ad19bcc2808c",
   "metadata": {},
   "source": [
    "### Obtaining a Transcript using Amazon Transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706681b8-af51-44a2-b7d1-ca39e8d20b71",
   "metadata": {},
   "source": [
    "In this section we will start a Transcribe job using the specified video file as the input. \n",
    "This will output the transcription as well as the SRT subtitles file to a folder on the S3 bucket called 'transcribeOutput'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f673e-3f67-4045-92aa-239b89170a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_transcribe_job(media_file_uri, output_bucket, output_key, language_code='en-US'):\n",
    "    transcribe = boto3.client('transcribe')\n",
    "    job_name = 'TranscribeJob-' + jobID\n",
    "    response = transcribe.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': media_file_uri},\n",
    "        MediaFormat='mp4',  # Change this to the format of your media file\n",
    "        OutputBucketName=output_bucket,\n",
    "        OutputKey=output_key,\n",
    "        LanguageCode=language_code,\n",
    "        Subtitles={\n",
    "        'Formats': [\n",
    "            'vtt','srt'\n",
    "        ],\n",
    "        'OutputStartIndex': 1\n",
    "        },\n",
    "        ToxicityDetection=[\n",
    "        {\n",
    "            'ToxicityCategories': [\n",
    "                'ALL',\n",
    "            ]\n",
    "        }\n",
    "        ] \n",
    "    )\n",
    "\n",
    "    print(\"Transcription job created:\")\n",
    "    print(response)\n",
    "    return job_name\n",
    "\n",
    "def wait_until_transcription_job_completed(job_name):\n",
    "    transcribe = boto3.client('transcribe')\n",
    "\n",
    "    while True:\n",
    "        response = transcribe.get_transcription_job(TranscriptionJobName=job_name)\n",
    "        status = response['TranscriptionJob']['TranscriptionJobStatus']\n",
    "\n",
    "        if status in ['COMPLETED', 'FAILED']:\n",
    "            print(f\"Transcription job {job_name} {status.lower()}.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Transcription job {job_name} is still in progress. Checking again in 30 seconds...\")\n",
    "        time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f6e206-0510-429d-b7e6-6bab6bf30e9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_bucket = bucket_name\n",
    "output_key = \"transcribeOutput/transcribe-\" + jobID\n",
    "job_name=create_transcribe_job(media_file_uri, output_bucket, output_key)\n",
    "wait_until_transcription_job_completed(job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73af176-3811-45f7-9391-7bd59ca9c06b",
   "metadata": {},
   "source": [
    "Once the transcription job is completed, we can retrieve the SRT subtitles file and the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342d468-6d09-496e-81f5-bf73a4880cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_from_s3(bucket_name, object_key, local_path):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    try:\n",
    "        s3.download_file(bucket_name, object_key, local_path)\n",
    "        print(f\"File downloaded successfully from S3: {local_path}\")\n",
    "    except:\n",
    "        print(\"File not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b047b-521d-424d-a8a3-61570a3a579a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "object_key=output_key + \".srt\"\n",
    "srt_file_path = \"output-transcribe.srt\"\n",
    "download_from_s3(output_bucket, object_key, srt_file_path)\n",
    "\n",
    "transcribe_json = \"output-transcribe-text.json\"\n",
    "download_from_s3(output_bucket, output_key, transcribe_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7045a7dd-89e3-48f4-8255-caa1d09619ab",
   "metadata": {},
   "source": [
    "We now have the SRT (subtitles with timestamps) and the JSON file of the transcription itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cdf17-afc7-46d0-b2b0-77be20952ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### now lets get the transcription \n",
    "with open(transcribe_json, 'r') as file:\n",
    "    data = json.load(file)\n",
    "transcript = data.get('results', {}).get('transcripts', [{}])[0].get('transcript', None)\n",
    "\n",
    "# Print the transcript\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec1a8e-13f3-4662-8627-cb4243767547",
   "metadata": {},
   "source": [
    "The following function will parse an SRT file and determine all the points where speech occurs. It will populate a dictionary every 500ms describing whether or not speech occurs. It will store this as a dictonary that we can use later for determining where breaks in the content are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95609039-1380-4a53-8ea2-37fe9ff3b7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Define a regular expression pattern to extract timestamps and text\n",
    "    pattern = re.compile(r'(\\d+:\\d+:\\d+,\\d+) --> (\\d+:\\d+:\\d+,\\d+)\\n(.+?)\\n\\n', re.DOTALL)\n",
    "\n",
    "    # Find all matches in the content\n",
    "    matches = re.findall(pattern, content)\n",
    "\n",
    "    # Create a dictionary to store whether text appears every 0.5 seconds\n",
    "    time_dict = {}\n",
    "\n",
    "    # Process matches and update the dictionary\n",
    "    for match in matches:\n",
    "        start_time, end_time, _ = match\n",
    "        start_seconds = convert_to_seconds(start_time)\n",
    "        end_seconds = convert_to_seconds(end_time)\n",
    "\n",
    "        current_time = start_seconds\n",
    "        while current_time < end_seconds:\n",
    "            # Round to the nearest 0.5 seconds\n",
    "            time_key = round(current_time * 2) / 2\n",
    "            time_dict.setdefault(time_key, True)\n",
    "            current_time += 0.5\n",
    "\n",
    "    # Iterate over every 0.5 seconds and set values to False if no text appears\n",
    "    total_duration = convert_to_seconds(matches[-1][1])  # Duration of the entire video\n",
    "    for time_key in range(0, int(total_duration * 2) + 1):\n",
    "        time_key /= 2\n",
    "        if time_key not in time_dict:\n",
    "            time_dict[time_key] = False\n",
    "\n",
    "    sorted_time_dict = dict(sorted(time_dict.items()))\n",
    "    df = pd.DataFrame(list(sorted_time_dict.items()), columns=['Time', 'Speech Appears'])\n",
    "    df['Speech Appears'] = df['Speech Appears'].map(lambda x: '1' if x else '0')\n",
    "\n",
    "\n",
    "    return df \n",
    "\n",
    "\n",
    "def convert_to_seconds(time_str):\n",
    "    # Convert timestamp to seconds, including milliseconds\n",
    "    h, m, s, ms = map(int, time_str.replace(',', ':').split(':'))\n",
    "    return h * 3600 + m * 60 + s + ms / 1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd50585-a244-4e7c-815c-22f559e5b447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcribe_pd = parse_file(srt_file_path)\n",
    "# Print the dictionary\n",
    "print(\"Time Dictionary:\")\n",
    "print (transcribe_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0029e-57d9-40ed-89a7-f81aa8af200e",
   "metadata": {},
   "source": [
    "### Analyse Transcript using a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a23a57-ae07-489e-8e4d-12fb75993de5",
   "metadata": {},
   "source": [
    "In this stage we will use Bedrock to analyse the transcript and determine what the content is about and what tags might be most appropriate to apply to the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e03bbf-0b38-4222-b961-133207fb04d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_text_response_from_bedrock(text, model_id):    \n",
    "    llm = Bedrock(model_id=model_id, region_name='us-east-1')\n",
    "    prompt = PromptTemplate(\n",
    "    template=\"\"\"Human:\n",
    "                    {text}\n",
    "                    Assistant: \"\"\",\n",
    "                    input_variables=[\"text\"],\n",
    "    )\n",
    "    llmchain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    response=llmchain.invoke({\"text\":text})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72189fd-3298-443d-b176-c486245ca2ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = get_text_response_from_bedrock('Here is a transcript from a video: \\n\\n' \n",
    "                                        + str(transcript) + \n",
    "                                        '\\n\\n Analyse the transcript and determine what type of video it is and what is happening. '\n",
    "                                        ,'anthropic.claude-v2:1')\n",
    "transcriptllm=answer['text']\n",
    "transcriptllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeaa38f-e838-480d-a162-2a0640472220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = get_text_response_from_bedrock(' Here is the transcript from the video: \\n\\n' \n",
    "                                        + str(transcript) + \n",
    "                                        '\\n\\n What are the top three keywords you would use for the content above. Output as a comma seperated list.'\n",
    "                                        ,'anthropic.claude-v2:1')\n",
    "tagsllm=answer['text']\n",
    "tagsllm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876555e3-02c0-4a5b-8479-2c964a4d5efe",
   "metadata": {},
   "source": [
    "### Using Amazon Rekognition Shot Detection to determine changes in scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9c98b-9137-4c21-b6e2-6d0f615ff974",
   "metadata": {},
   "source": [
    "In this step we will use the Shot Dection feature of Amazon Rekognition to determine where the scene changes. \n",
    "This is used to firstly ensure we have a screen shot of every scene to generate a caption for. Secondly it is used to determine where there might be a break in the content. For the breaks we'll round this to the nearest 0.5 second to match with the speech we detected above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d642e-067c-42f5-b9ed-cb145789dc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def round_down_to_nearest_half_second(timestamp_millis):\n",
    "    # Convert milliseconds to seconds and round down to the nearest 0.5 seconds\n",
    "    rounded_seconds = math.floor(timestamp_millis / 1000.0 * 2) / 2.0\n",
    "    return float(rounded_seconds)\n",
    "\n",
    "def populate_segment_indicator_dict(results):\n",
    "    segment_indicator_dict = {}\n",
    "    duration=results['VideoMetadata'][0]['DurationMillis']\n",
    "    duration=round_down_to_nearest_half_second(duration)\n",
    "    segments=results['Segments'] \n",
    "    \n",
    "    current=0\n",
    "    while current < duration:\n",
    "        segment_indicator_dict[current]=0 \n",
    "        current=current+0.5\n",
    "    \n",
    "    for segment in segments:\n",
    "        start_time=segment['StartTimestampMillis']\n",
    "        start_time_segment = round_down_to_nearest_half_second(segment['StartTimestampMillis'])\n",
    "        segment_indicator_dict[start_time_segment]=start_time\n",
    "    return segment_indicator_dict\n",
    "\n",
    "rekognition_client = boto3.client('rekognition')\n",
    "\n",
    "def detect_video_segments(bucket_name, video_file, interval=0.5):\n",
    "\n",
    "    # Start segment detection\n",
    "    response = rekognition_client.start_segment_detection(\n",
    "        Video={\n",
    "            'S3Object': {\n",
    "                'Bucket': bucket_name,\n",
    "                'Name': video_file\n",
    "            }\n",
    "        },\n",
    "        Filters={\n",
    "             'ShotFilter': {\n",
    "            'MinSegmentConfidence': 70\n",
    "        }\n",
    "\n",
    "        },\n",
    "        SegmentTypes=['SHOT'] \n",
    "    )\n",
    "\n",
    "    job_id = response['JobId']\n",
    "    return job_id\n",
    "\n",
    "def get_results(job_id):\n",
    "    # Wait for segment detection to complete\n",
    "    while True:\n",
    "        result = rekognition_client.get_segment_detection(JobId=job_id)\n",
    "\n",
    "        if result['JobStatus'] in ['SUCCEEDED', 'FAILED']:\n",
    "            break\n",
    "        time.sleep(5)  # Wait for 5 seconds before checking again\n",
    "\n",
    "    # Analyze segment results\n",
    "    segments = result\n",
    "    segment_indicator_dict = populate_segment_indicator_dict(segments)\n",
    "\n",
    "\n",
    "    return segment_indicator_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fc5b5-fc89-4047-8bcc-59b783418c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rek_job_id = detect_video_segments(bucket_name, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e94d4b-5275-4634-98f5-4838b5863a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment_indicator_dict = get_results(rek_job_id)\n",
    "rekognition_pd = pd.DataFrame(list(segment_indicator_dict.items()), columns=['Time', 'Shot Transition'])\n",
    "print(rekognition_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87bdef-ce55-4931-b9ef-cf6032a14d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transition_frames_ms = rekognition_pd[rekognition_pd['Shot Transition'] != 0]['Shot Transition'].values\n",
    "\n",
    "# Display the array\n",
    "print(transition_frames_ms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32becfc5-b55d-4203-b02f-948f0915a716",
   "metadata": {},
   "source": [
    "Now we have the frames (milliseconds) where there is a transition and we'd want to generate a caption for each one. \n",
    "\n",
    "Firstly, we'll remove any contents of the screenshots folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a2391-7b9a-460f-9a39-6fc86e0b22eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%mkdir screenshots\n",
    "%rm -Rf screenshots/* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b92fc80-eab3-46cc-b441-9e90575b552e",
   "metadata": {},
   "source": [
    "Download the video from S3 for local processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0282652c-e27f-4a94-b833-b94bf41643b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "download_from_s3(bucket_name, key, key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b2b01-4fe2-4419-8fd7-02bfc11acfcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_frame(video_path, frame_milliseconds, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Set the frame position to the desired milliseconds\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, frame_milliseconds)\n",
    "\n",
    "    # Read the frame at the specified position\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # If the frame is successfully read, save it\n",
    "    if success:\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        print(f\"Frame at {frame_milliseconds} milliseconds extracted and saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to extract frame at {frame_milliseconds} milliseconds\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf0eae-2a40-46ce-ae5f-e00fcb76e673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for frame_milliseconds in transition_frames_ms:\n",
    "    # add 10ms to ensure into the new scene \n",
    "    frame_milliseconds=frame_milliseconds+10 \n",
    "    \n",
    "    output_path = 'screenshots/frame_' + str(frame_milliseconds) + '.jpg'\n",
    "    extract_frame(key, frame_milliseconds, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134dbb22-9f64-44ac-a097-2674e7977fb8",
   "metadata": {},
   "source": [
    "## Use Claude 3 to create a caption for each scene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d7c33",
   "metadata": {},
   "source": [
    "By using Amazon Bedrock with the Claude 3 Multi-Modal model, it is possible to generate a caption for every shot captured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45272e17-1902-489e-b482-2879d1d3893b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_image(img_file):\n",
    "    with open(img_file, \"rb\") as image_file:\n",
    "        img_str = base64.b64encode(image_file.read())\n",
    "        base64_string = img_str.decode(\"utf-8\")\n",
    "    return base64_string\n",
    "\n",
    "def run_inference(bedrock_runtime, model_id, messages, max_tokens):\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "             \"messages\": messages\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    return response_body[\"content\"][0][\"text\"] \n",
    "\n",
    "\n",
    "def process_files_in_folder(folder_path):\n",
    "    captions={}\n",
    "    try:\n",
    "        # List all files in the specified folder\n",
    "        files = os.listdir(folder_path)\n",
    "\n",
    "        # Loop through each file in the folder\n",
    "        for file_name in files:\n",
    "            # Full path to the file\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Check if it's a file (not a subfolder)\n",
    "            if os.path.isfile(file_path):\n",
    "                # Perform your action here\n",
    "                base64_string = encode_image(file_path)\n",
    "                message = {\"role\": \"user\",\n",
    "                 \"content\": [\n",
    "                    {\"type\": \"image\", \"source\": {\"type\": \"base64\",\n",
    "                    \"media_type\": \"image/jpeg\", \"data\": base64_string}},\n",
    "                    {\"type\": \"text\", \"text\": \"What is happening in this image?\"}\n",
    "                    ]}\n",
    "\n",
    "                match = re.search(r'frame_(\\d+)', file_path)\n",
    "\n",
    "                if match:\n",
    "                    extracted_number = float(match.group(1))\n",
    "                else: \n",
    "                    extracted_number=\"\"\n",
    "                seconds_timestamp=round_down_to_nearest_half_second(extracted_number)\n",
    "                bedrock_runtime = boto3.client(service_name='bedrock-runtime')\n",
    "                model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "                messages = [message]\n",
    "                caption=run_inference(bedrock_runtime,model_id,messages, 100)\n",
    "                captions[seconds_timestamp]=caption\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Folder not found - {folder_path}\")\n",
    "    return captions \n",
    "\n",
    "captions=process_files_in_folder('screenshots')\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "\n",
    "captions_df = pd.DataFrame(list(captions.items()), columns=['Time', 'Captions'])\n",
    "captions_df = captions_df.sort_values(by='Time')\n",
    "\n",
    "listofcaptions=captions_df['Captions'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588e801-2067-4a8e-80c5-44e10826efb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "listofcaptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd31794-43eb-45af-9c9c-2dc583134a85",
   "metadata": {},
   "source": [
    "We want a caption for every 0.5s, so we need to 'infill' the gaps with the previous caption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fb8994-4e33-400f-afd5-4402696b0343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "captions_df['Time'] = pd.to_numeric(captions_df['Time'])\n",
    "captions_df = captions_df.sort_values(by='Time')\n",
    "\n",
    "last_row=captions_df.iloc[-1]\n",
    "last_row=float(last_row['Time']) \n",
    "\n",
    "time_values = [i * 0.5 for i in range(0, int(last_row * 2) + 1)]\n",
    "df = pd.DataFrame({'Time': time_values})\n",
    "captions_df = pd.merge(df, captions_df, on='Time', how='left')\n",
    "captions_df['Captions'].fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81e1a7-61f1-4ac8-8b33-f9b4a259bb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "captions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abcaa4-8d1f-47a8-b4a7-6bf5d2779f4f",
   "metadata": {},
   "source": [
    "### Use Bedrock to analyse the captions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a69a5-634b-471f-bad8-3bb9e84dd0a4",
   "metadata": {},
   "source": [
    "We can now use an LLM via Bedrock to analyse what is happening in the video based on its captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be448fd-0c28-4f41-9c8b-ab78b8a65663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "answer = get_text_response_from_bedrock('Here is a list of captions of what is happening in a video: \\n\\n' \n",
    "                                        + str(listofcaptions) + \n",
    "                                        '\\n\\n Summarise what is happening.'\n",
    "                                        ,'anthropic.claude-v2:1')\n",
    "captionllm=answer['text']\n",
    "captionllm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b1592-f8ad-4a0a-89dc-a160e83a0f89",
   "metadata": {},
   "source": [
    "### Combining captions and transcription "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba5181-c4c4-4d8c-af87-a6f80b6893a3",
   "metadata": {},
   "source": [
    "Next we can combine the captions with the transcript to create a richer view of what is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1097b6-e7a0-428c-910e-1093cb697ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "answer = get_text_response_from_bedrock('Here is a list of captions of what is happening in a video: \\n\\n' \n",
    "                                        + str(listofcaptions) + \n",
    "                                        '\\n\\n Here is the transcript from the video: \\n\\n' \n",
    "                                        + str(transcript) + \n",
    "                                        '\\n\\n What is happening in the video?'\n",
    "                                        ,'anthropic.claude-v2:1')\n",
    "tagsllm=answer['text']\n",
    "tagsllm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99777f8-4a4e-450d-bd29-ddc937e41f35",
   "metadata": {},
   "source": [
    "### Gather volume levels from the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3580d7-01f9-42c3-819d-8d5588f7497e",
   "metadata": {},
   "source": [
    "We can use the librosa library to extract the volume levels for the video every 0.5s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2304a9-3385-448b-afbf-9ed178b32ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_volume_levels(audio_file_path):\n",
    "    # Load audio file using librosa\n",
    "    audio, sr = librosa.load(audio_file_path, sr=None)\n",
    "\n",
    "    # Calculate volume levels every 500 milliseconds\n",
    "    frame_size = int(sr / 2)  # 500 milliseconds\n",
    "    num_frames = len(audio) // frame_size\n",
    "\n",
    "    volume_data = {\"Time\": [], \"Volume\": []}\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame = audio[i * frame_size: (i + 1) * frame_size]\n",
    "        time_in_seconds = i * 0.5  # 500 milliseconds is 0.5 seconds\n",
    "\n",
    "        volume_level = np.mean(np.abs(frame))\n",
    "        normalized_volume = int((volume_level / np.max(audio)) * 255)\n",
    "        \n",
    "        volume_data[\"Time\"].append(time_in_seconds)\n",
    "        volume_data[\"Volume\"].append(normalized_volume)\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df = pd.DataFrame(volume_data)\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8d22e",
   "metadata": {},
   "source": [
    "#### Extract Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a593e967",
   "metadata": {},
   "source": [
    "If you get errors referring to formats, please ensure you have installed ffmpeg. \n",
    "Depending on your input file format, you may get warnings about using a different module to read the audio file. You may continue to the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcb9f1-e100-4c12-989d-46f53198e9d8",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "volume_df=extract_volume_levels(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4428c8-3d16-48e0-8260-b219d68ec7ec",
   "metadata": {},
   "source": [
    "### Merge the data together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405e0b6-3ed5-42eb-9ef5-bbd3d6e7b240",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have data on the shot transitions, the captions and whether speech is appearing, we can combine this into a single source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2f898-1782-4ecd-8689-0a3cb2e21c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df=pd.merge(rekognition_pd, transcribe_pd, on='Time', how='outer')\n",
    "merged_df=pd.merge(merged_df, volume_df, on='Time', how='outer')\n",
    "merged_df=pd.merge(merged_df, captions_df, on='Time', how='outer')\n",
    "\n",
    "merged_df = merged_df.fillna(0)\n",
    "csvkey='analysis-'+ key + '.csv'\n",
    "merged_df.to_csv(csvkey, index=False)\n",
    "merged_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabedf1-e9c2-4450-817e-bacce1c4974a",
   "metadata": {},
   "source": [
    "### Create a score for each 0.5s for suitability for a break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506fbb4-d7cb-407f-a25e-6c4d29f89cc1",
   "metadata": {},
   "source": [
    "There is some further analysis we can do on this data. We can determine whether a 0.5s interval is in the quietest 10% of the video, and also do this on a sliding window of every 30 seconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81114e-3212-4782-8bea-7e8a3400d491",
   "metadata": {},
   "source": [
    "We also invert some of the data so it is a binary 'yes or no' and then create a rudimentary score for every 0.5s as to its suitability for a break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece53795-1716-44d9-8e2b-2588a443478d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sliding_label_bottom_10_percent(df):\n",
    "    total_rows = len(df)\n",
    "    bottom_10_percent = int(0.1 * total_rows)\n",
    "\n",
    "    # Create a new column 'Bottom_10_Percent' and initialize with False\n",
    "    df['sliding_quiet'] = 0\n",
    "\n",
    "    # Iterate through every 60 data points and label the lowest 10% as True\n",
    "    for i in range(0, total_rows, 60):\n",
    "        subset = df.iloc[i:i+60]  # Select every 60 data points\n",
    "        bottom_10_subset = subset.nsmallest(6, 'Volume')  # Select the lowest 10%\n",
    "        df.loc[bottom_10_subset.index, 'sliding_quiet'] = 1\n",
    "    return df \n",
    "\n",
    "analysis_df=merged_df \n",
    "\n",
    "# Turn the RMS into a postive number \n",
    "analysis_df['Volume'] = analysis_df['Volume']\n",
    "\n",
    "# Invert Speech Appears so its a 0 when there is silence \n",
    "analysis_df['No Speech'] = 1 - analysis_df['Speech Appears'].astype(int)\n",
    "\n",
    "# Shot Transition is to be a 1 or a 0 \n",
    "analysis_df['Shot Transition Binary'] = analysis_df['Shot Transition'].apply(lambda x: 1 if x != 0.0 else 0)\n",
    "\n",
    "# now find the lowest 10% of the RMS values \n",
    "lowest_10_percent = analysis_df['Volume'].quantile(0.1)\n",
    "analysis_df['quiet'] = analysis_df['Volume'].apply(lambda x: 1 if x <= lowest_10_percent else 0)\n",
    "\n",
    "#### get the sliding 10%\n",
    "analysis_df=sliding_label_bottom_10_percent(analysis_df)\n",
    "\n",
    "analysis_df['Break Score'] = analysis_df['Shot Transition Binary'] + analysis_df['No Speech'] + analysis_df['quiet']+ analysis_df['sliding_quiet']\n",
    "\n",
    "statscsvkey='stats-analysis-'+ key + '.csv'\n",
    "analysis_df.to_csv(statscsvkey, index=False)\n",
    "analysis_df.set_index('Time', inplace=True)\n",
    "\n",
    "analysis_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2914b-abd8-47de-8201-c587b562ae8c",
   "metadata": {},
   "source": [
    "### Optional: Overlay the data onto the video by using SRT captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa1896-6e91-4d88-8365-0461f3312f10",
   "metadata": {},
   "source": [
    "Having the data in textual format every 0.5s is useful and can be used for determinig where to break the content and also for tagging the video.\n",
    "However, for experimentation purposes it is useful to be able to visualise the data we have captured.\n",
    "\n",
    "For this, we will overlay the data using SRT captions onto the video using Amazon Elemental MediaConvert.\n",
    "\n",
    "You can obtain your MediaConvert API Endpoint URL from the AWS Elemental MediaConvert console under \"Account\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649f9f4-ea0f-4152-ae3b-5133f1cce0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "media_convert_endpoint='https://xxxxxxxx.mediaconvert.us-east-1.amazonaws.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409e1c5",
   "metadata": {},
   "source": [
    "You will need to create an IAM role for MediaConvert. If using the IAM console, when creating a IAM role, if the \"Use Case\" is selected as \"MediaConvert\" the default permissions will allow access to your S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_convert_arn='arn:aws:iam::xxxxxxxxx:role/service-role/MediaConvert_Default_Role'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2695d34",
   "metadata": {},
   "source": [
    "You will then need to amend the role your are using to execute this notebook to allow the \"iam:PassRole\" action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc38d31-166b-40a9-92a1-d5cbf9e5f75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seconds_to_srt_time(seconds):\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    milliseconds = int((seconds - int(seconds)) * 1000)\n",
    "    formatted_time = \"{:02}:{:02}:{:02},{:03}\".format(int(hours), int(minutes), int(seconds), milliseconds)\n",
    "    return formatted_time\n",
    "\n",
    "\n",
    "def csv_to_srt(csv_file_path, srt_file_path='output.srt'):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Create an SRT file\n",
    "    with open(srt_file_path, 'w') as srt_file:\n",
    "        for index, row in df.iterrows():\n",
    "            start_time = seconds_to_srt_time(row['Time']) \n",
    "            end_time = seconds_to_srt_time(row['Time']+0.5) \n",
    "\n",
    "            # Format time as HH:MM:SS,mmm\n",
    "            srt_file.write(f\"{index+1}\\n\")\n",
    "            srt_file.write(f\"{start_time} --> {end_time}\\n\")\n",
    "            volume=int(row['Volume'])\n",
    "\n",
    "            text_to_display='' \n",
    "\n",
    "            text_to_display += ' Break Score: ' + str(row['Break Score']) + '/4\\n' \n",
    "                \n",
    "            if row['Speech Appears'] == 1:\n",
    "                text_to_display += ' ** SPEECH ** \\n'\n",
    "            else: \n",
    "                text_to_display += ' - \\n' \n",
    "            if row['Shot Transition'] != 0:\n",
    "                text_to_display += ' ** SHOT TRANSITION ** ' + str(row['Shot Transition']) +'\\n'\n",
    "            else:\n",
    "                text_to_display += ' - \\n'\n",
    "            if row['Volume']:\n",
    "                text_to_display += ' VOLUME: ' + str(volume) + '\\n' \n",
    "            else:\n",
    "                text_to_display += ' 0\\n'\n",
    "            if row['Captions']:\n",
    "                text_to_display += ' C: ' + row['Captions'] + '\\n' \n",
    "            else:\n",
    "                text_to_display += ' No captions\\n'\n",
    "\n",
    "            srt_file.write(f\"{text_to_display}\\n\\n\")\n",
    "\n",
    "\n",
    "    print(f\"SRT file '{srt_file_path}' has been created.\")\n",
    "\n",
    "csv_to_srt(statscsvkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e469e-5fcb-41cd-a527-bed94f4baefb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_to_s3(file_path, bucket_name, object_name):\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket_name, object_name)\n",
    "        print(f\"File uploaded to S3: s3://{bucket_name}/{object_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "srt_file_path = 'output.srt'  # Replace with the actual path to your SRT file\n",
    "s3_bucket_name = bucket_name\n",
    "s3_object_name = 'output.srt'\n",
    "\n",
    "upload_to_s3(srt_file_path, s3_bucket_name, s3_object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f525580-cf60-4186-bbda-dd23f428c17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mediaconvert_job_captions(media_file_uri,bucket_name):\n",
    "\n",
    "\n",
    "    # Initialize a MediaConvert client\n",
    "    mediaconvert = boto3.client('mediaconvert', endpoint_url=media_convert_endpoint)\n",
    "    jobinputs ={\n",
    "    \"OutputGroups\": [\n",
    "      {\n",
    "        \"Name\": \"File Group\",\n",
    "        \"Outputs\": [\n",
    "          {\n",
    "            \"ContainerSettings\": {\n",
    "              \"Container\": \"MP4\",\n",
    "              \"Mp4Settings\": {}\n",
    "            },\n",
    "            \"VideoDescription\": {\n",
    "              \"CodecSettings\": {\n",
    "                \"Codec\": \"H_264\",\n",
    "                \"H264Settings\": {\n",
    "                  \"MaxBitrate\": 1000000,\n",
    "                  \"RateControlMode\": \"QVBR\",\n",
    "                  \"SceneChangeDetect\": \"TRANSITION_DETECTION\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"AudioDescriptions\": [\n",
    "              {\n",
    "                \"AudioSourceName\": \"Audio Selector 1\",\n",
    "                \"CodecSettings\": {\n",
    "                  \"Codec\": \"AAC\",\n",
    "                  \"AacSettings\": {\n",
    "                    \"Bitrate\": 96000,\n",
    "                    \"CodingMode\": \"CODING_MODE_2_0\",\n",
    "                    \"SampleRate\": 48000\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            \"CaptionDescriptions\": [\n",
    "              {\n",
    "                \"CaptionSelectorName\": \"Captions Selector 1\",\n",
    "                \"DestinationSettings\": {\n",
    "                  \"DestinationType\": \"BURN_IN\",\n",
    "                  \"BurninDestinationSettings\": {\n",
    "                    \"FontSize\": 14,\n",
    "                    \"FontColor\": \"RED\",\n",
    "                    \"BackgroundColor\": \"WHITE\"\n",
    "\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ],\n",
    "        \"OutputGroupSettings\": {\n",
    "          \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "          \"FileGroupSettings\": {\n",
    "            \"Destination\": \"s3://\"+bucket_name+\"/outputVideos/\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"FollowSource\": 1,\n",
    "    \"Inputs\": [\n",
    "      {\n",
    "        \"AudioSelectors\": {\n",
    "          \"Audio Selector 1\": {\n",
    "            \"Tracks\": [\n",
    "              1\n",
    "            ],\n",
    "            \"DefaultSelection\": \"DEFAULT\",\n",
    "            \"SelectorType\": \"TRACK\"\n",
    "          }\n",
    "        },\n",
    "        \"VideoSelector\": {},\n",
    "        \"TimecodeSource\": \"ZEROBASED\",\n",
    "        \"CaptionSelectors\": {\n",
    "          \"Captions Selector 1\": {\n",
    "            \"SourceSettings\": {\n",
    "              \"SourceType\": \"SRT\",\n",
    "              \"FileSourceSettings\": {\n",
    "                \"SourceFile\": \"s3://\"+bucket_name+\"/output.srt\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"FileInput\": media_file_uri\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "\n",
    "    # Create a MediaConvert job\n",
    "    response = mediaconvert.create_job(\n",
    "        Role=media_convert_arn,\n",
    "        Settings=jobinputs\n",
    "    )\n",
    "    \n",
    "    print(\"MediaConvert job created successfully.\")\n",
    "    print(\"Job ID:\", response['Job']['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edfc815-41d9-4e0e-9c27-fcbcc152417e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_mediaconvert_job_captions(media_file_uri,bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42f5085-5e79-4891-b9d4-aa5b9cf1d98c",
   "metadata": {},
   "source": [
    "Once this job is complete, you'll be able to see your original video with our analysis overlayed. It will be in your bucket in the 'outputVideos' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c96fce-fe46-4c52-b413-bce42ec60ccd",
   "metadata": {},
   "source": [
    "## Clean Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc279a4",
   "metadata": {},
   "source": [
    "The first step is to remove the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9fa51a-e1f0-418b-b855-e6d473030ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=model_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e18f0",
   "metadata": {},
   "source": [
    "Next, you should remove any IAM roles you have created.\n",
    "\n",
    "The S3 bucket you utilised to store your videos and the analysis files can be removed if desired.\n",
    "\n",
    "The video file and analysis outputs will also have been downloaded locally for volume analysis and can be removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbc75e-48f6-4f2f-b68a-93677c4fad10",
   "metadata": {},
   "source": [
    "## Additional Code: Compressing Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a9ba7-3d9a-4169-b355-7e3287ed14dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you wish, you can use the following code as a first step to compress your video file using the AWS Elemental MediaConvert service. This will create a compressed version of your video in the original S3 bucket with the suffix \"-compressed\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fa3ee-8048-48fc-8c6f-3ddc50f308a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mediaconvert_job_compress(media_file_uri,bucket_name):\n",
    "\n",
    "\n",
    "    # Initialize a MediaConvert client\n",
    "    mediaconvert = boto3.client('mediaconvert', endpoint_url=media_convert_endpoint)\n",
    "    jobinputs ={\n",
    "      \"OutputGroups\": [\n",
    "      {\n",
    "        \"Name\": \"File Group\",\n",
    "        \"Outputs\": [\n",
    "          {\n",
    "            \"ContainerSettings\": {\n",
    "              \"Container\": \"MP4\",\n",
    "              \"Mp4Settings\": {}\n",
    "            },\n",
    "            \"VideoDescription\": {\n",
    "              \"Height\": 720,\n",
    "              \"CodecSettings\": {\n",
    "                \"Codec\": \"H_264\",\n",
    "                \"H264Settings\": {\n",
    "                  \"MaxBitrate\": 1000000,\n",
    "                  \"RateControlMode\": \"QVBR\",\n",
    "                  \"SceneChangeDetect\": \"TRANSITION_DETECTION\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"AudioDescriptions\": [\n",
    "              {\n",
    "                \"CodecSettings\": {\n",
    "                  \"Codec\": \"AAC\",\n",
    "                  \"AacSettings\": {\n",
    "                    \"Bitrate\": 96000,\n",
    "                    \"CodingMode\": \"CODING_MODE_2_0\",\n",
    "                    \"SampleRate\": 48000\n",
    "                  }\n",
    "                }\n",
    "              }\n",
    "            ],\n",
    "            \"NameModifier\": \"-compressed\"\n",
    "          }\n",
    "        ],\n",
    "        \"OutputGroupSettings\": {\n",
    "          \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "          \"FileGroupSettings\": {\n",
    "            \"Destination\": \"s3://\" + bucket_name + \"/\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"FollowSource\": 1,\n",
    "    \"Inputs\": [\n",
    "      {\n",
    "        \"AudioSelectors\": {\n",
    "          \"Audio Selector 1\": {\n",
    "            \"DefaultSelection\": \"DEFAULT\"\n",
    "          }\n",
    "        },\n",
    "        \"VideoSelector\": {},\n",
    "        \"TimecodeSource\": \"ZEROBASED\",\n",
    "        \"FileInput\": media_file_uri\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "    # Create a MediaConvert job\n",
    "    response = mediaconvert.create_job(\n",
    "        Role=media_convert_arn,\n",
    "        Settings=jobinputs\n",
    "    )\n",
    "    return response['Job']['Id']\n",
    "\n",
    "def check_mediaconvert_job_status(job_id):\n",
    "    mediaconvert = boto3.client('mediaconvert', endpoint_url=media_convert_endpoint)\n",
    "\n",
    "    while True:\n",
    "        response = mediaconvert.get_job(Id=job_id)\n",
    "        job_status = response['Job']['Status']\n",
    "\n",
    "        print(f\"Job ID: {job_id}, Status: {job_status}\")\n",
    "\n",
    "        if job_status in ['COMPLETE', 'ERROR']:\n",
    "            break\n",
    "\n",
    "        time.sleep(30)  # Sleep for 30 seconds before checking again\n",
    "\n",
    "    if job_status == 'COMPLETE':\n",
    "        print(\"MediaConvert job completed successfully.\")\n",
    "    else:\n",
    "        print(\"MediaConvert job failed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876fe8c-2fe4-4dea-8cbb-9b33107876bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobid=create_mediaconvert_job_compress(media_file_uri,bucket_name)\n",
    "check_mediaconvert_job_status(jobid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
